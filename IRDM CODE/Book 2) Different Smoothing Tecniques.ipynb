{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Book 2) Different Smoothing Tecniques.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1tbZ-W7by-5hfFiFjdmlzFCj3y4EQJ3td","timestamp":1523156933001},{"file_id":"1__ng1Z7yYZp4hbOI6uO6IkesnMhTqT9b","timestamp":1522852802900}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"jMUOotU2BIFy","colab_type":"text"},"cell_type":"markdown","source":["## NIREN PATEL IRDM STANCE DETECTION BOOK 2"]},{"metadata":{"id":"VT6vKNmQSlar","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#!kill -9 -1 # Kill Switch Do not run this cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j1k8Trbg6R8Y","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"v2ukDFwUamw6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LfH1e162aKGq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Code to read csv file into colaboratory:\n","!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","#2. Get the file\n","TrainProcessed = drive.CreateFile({'id':'12IgQVJPo-oznDjoy4qp-yBEv0wvjx-6I'}) \n","TrainProcessed.GetContentFile('TrainProcessed.csv')  \n","\n","ValidationProcessed = drive.CreateFile({'id':'1ao3ZMAwle7W9ZBPZ-lVPHmqFlhMHJ9hs'}) \n","ValidationProcessed.GetContentFile('ValidationProcessed.csv')\n","\n","TestProcessed = drive.CreateFile({'id':'1sOMZL3UQbnJXOxrDoQDBpm4a8GNeACaH'}) \n","TestProcessed.GetContentFile('TestProcessed.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pbkMNXbEdJeI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":51},"outputId":"09a5b4d0-b183-4454-b565-bb69e5811590","executionInfo":{"status":"ok","timestamp":1530097486576,"user_tz":-60,"elapsed":1513,"user":{"displayName":"niren52","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118152712183165316846"}}},"cell_type":"code","source":["#IMPORT PACKAGES\n","import numpy as np\n","import pandas as pd\n","import re\n","import math\n","from collections import Counter\n","import nltk\n","from nltk import word_tokenize\n","from nltk.tokenize import TweetTokenizer\n","nltk.download('punkt')\n","import string\n","from  itertools import chain\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /content/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"metadata":{"id":"411OWi1Ac3MM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Read file as panda dataframe\n","TrainProcessed = pd.read_csv('TrainProcessed.csv')\n","ValidationProcessed = pd.read_csv('ValidationProcessed.csv')\n","TestProcessed = pd.read_csv('TestProcessed.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"09KtCvyn6mk_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#PUNCTUATION REMOVAL\n","TrainProcessed['Headline']         =  TrainProcessed[\"Headline\"].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\n","TrainProcessed['articleBody']      =  TrainProcessed[\"articleBody\"].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\n","TestProcessed['Headline']          =  TestProcessed[\"Headline\"].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\n","TestProcessed['articleBody']       =  TestProcessed[\"articleBody\"].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\n","ValidationProcessed['Headline']    =  ValidationProcessed[\"Headline\"].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\n","ValidationProcessed['articleBody'] =  ValidationProcessed[\"articleBody\"].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M1B5NpjMwSAx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#TOKENISATION\n","TrainProcessed['Headline']         =  TrainProcessed[\"Headline\"].apply(nltk.word_tokenize)\n","TrainProcessed['articleBody']      =  TrainProcessed[\"articleBody\"].apply(nltk.word_tokenize)\n","TestProcessed['Headline']          =  TestProcessed[\"Headline\"].apply(nltk.word_tokenize)\n","TestProcessed['articleBody']       =  TestProcessed[\"articleBody\"].apply(nltk.word_tokenize)\n","ValidationProcessed['Headline']    =  ValidationProcessed[\"Headline\"].apply(nltk.word_tokenize)\n","ValidationProcessed['articleBody'] =  ValidationProcessed[\"articleBody\"].apply(nltk.word_tokenize)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SDRxVXfqA8_R","colab_type":"text"},"cell_type":"markdown","source":["# LAPLACE SMOOTHING"]},{"metadata":{"id":"2SidchlmBUMe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for QueryLieklihood\n","def QueryLikelihoodLaplaceSmoothing(dataset):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryHeadline = {}\n","    UniqueHeadline = []\n","    [UniqueHeadline.append(i) for i in row.Headline if not UniqueHeadline.count(i)]  \n","    \n","    for headlineWord in UniqueHeadline:\n","      DictionaryHeadline[headlineWord]=1 \n","      for bodyWord in row.articleBody:\n","        DictionaryHeadline[bodyWord]=1\n","\n","    for headlineWord in UniqueHeadline:\n","      if headlineWord in row.articleBody:\n","        countWord = row.articleBody.count(headlineWord)\n","        DictionaryHeadline[headlineWord] = DictionaryHeadline[headlineWord] + countWord\n","      else:\n","        pass\n","    TotalWords = sum(DictionaryHeadline.values())\n","    for key in DictionaryHeadline:\n","      DictionaryHeadline[key] /= TotalWords\n","    DictionaryList.append(DictionaryHeadline)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eDE0EhVKBUMi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for DocumentLikelihood\n","def DocumentLikelihoodLaplaceSmoothing(dataset):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryBody = {}\n","    UniqueBody = []\n","    [UniqueBody.append(i) for i in row.articleBody if not UniqueBody.count(i)]  \n","\n","    for bodyWord in UniqueBody:\n","      DictionaryBody[bodyWord]=1\n","      for headlineWord in row.Headline:\n","        DictionaryBody[headlineWord]=1 \n","\n","\n","    for bodyWord in UniqueBody:\n","      if bodyWord in row.Headline:\n","        countWord = row.Headline.count(bodyWord)\n","        DictionaryBody[bodyWord] = DictionaryBody[bodyWord] + countWord\n","      else:\n","        pass\n","    TotalWords = sum(DictionaryBody.values())\n","    for key in DictionaryBody:\n","      DictionaryBody[key] /= TotalWords\n","    DictionaryList.append(DictionaryBody)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bRrhzdn3BUMl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Get Prbability Distributions for each model\n","TrainQueryLikelihoodLaplace         = QueryLikelihoodLaplaceSmoothing(TrainProcessed)\n","ValidationQueryLikelihoodLaplace    = QueryLikelihoodLaplaceSmoothing(ValidationProcessed)\n","TestQueryLikelihoodLaplace          = QueryLikelihoodLaplaceSmoothing(TestProcessed)\n","\n","TrainDocumentLikelihoodLaplace      = DocumentLikelihoodLaplaceSmoothing(TrainProcessed)\n","ValidationDocumentLikelihoodLaplace = DocumentLikelihoodLaplaceSmoothing(ValidationProcessed)\n","TestDocumentLikelihoodLaplace       = DocumentLikelihoodLaplaceSmoothing(TestProcessed)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uGlWKegV2eiv","colab_type":"text"},"cell_type":"markdown","source":["# LIDSTONE CORRECTION"]},{"metadata":{"id":"AzsvO7CY-3ST","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for QueryLieklihood\n","def QueryLikelihoodDiscounting(dataset):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryHeadline = {}\n","    UniqueHeadline = []\n","    [UniqueHeadline.append(i) for i in row.Headline if not UniqueHeadline.count(i)]  \n","    \n","    for headlineWord in UniqueHeadline:\n","      DictionaryHeadline[headlineWord]=0.2\n","      for bodyWord in row.articleBody:\n","        DictionaryHeadline[bodyWord]=0.2\n","\n","    for headlineWord in UniqueHeadline:\n","      if headlineWord in row.articleBody:\n","        countWord = row.articleBody.count(headlineWord)\n","        DictionaryHeadline[headlineWord] = DictionaryHeadline[headlineWord] + countWord\n","      else:\n","        pass\n","    TotalWords = sum(DictionaryHeadline.values())\n","    for key in DictionaryHeadline:\n","      DictionaryHeadline[key] /= TotalWords\n","    DictionaryList.append(DictionaryHeadline)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nQE-SDs2Yg8T","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for DocumentLikelihood\n","def DocumentLikelihoodDiscounting(dataset):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryBody = {}\n","    UniqueBody = []\n","    [UniqueBody.append(i) for i in row.articleBody if not UniqueBody.count(i)]  \n","\n","    for bodyWord in UniqueBody:\n","      DictionaryBody[bodyWord]=0.2\n","      for headlineWord in row.Headline:\n","        DictionaryBody[headlineWord]=0.2\n","\n","\n","    for bodyWord in UniqueBody:\n","      if bodyWord in row.Headline:\n","        countWord = row.Headline.count(bodyWord)\n","        DictionaryBody[bodyWord] = DictionaryBody[bodyWord] + countWord\n","      else:\n","        pass\n","    TotalWords = sum(DictionaryBody.values())\n","    for key in DictionaryBody:\n","      DictionaryBody[key] /= TotalWords\n","    DictionaryList.append(DictionaryBody)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eaM8oBiMC8Tm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Get Prbability Distributions for each model\n","TrainQueryLikelihoodDiscounting         = QueryLikelihoodDiscounting(TrainProcessed)\n","ValidationQueryLikelihoodDiscounting    = QueryLikelihoodDiscounting(ValidationProcessed)\n","TestQueryLikelihoodDiscounting          = QueryLikelihoodDiscounting(TestProcessed)\n","\n","TrainDocumentLikelihoodDiscounting      = DocumentLikelihoodDiscounting(TrainProcessed)\n","ValidationDocumentLikelihoodDiscounting = DocumentLikelihoodDiscounting(ValidationProcessed)\n","TestDocumentLikelihoodDiscounting       = DocumentLikelihoodDiscounting(TestProcessed)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tov6entvAI9e","colab_type":"text"},"cell_type":"markdown","source":["# JELINEK-MERCER SMOOTHING"]},{"metadata":{"id":"uKaFd-EvZs6r","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#CREATE FREQUENCY DISTRIBUTION OF ALL WORDS FOR EACH LANGUAGE MODEL! FOR BACKGROUND NOISE\n","\n","TrainFrequencyHeadline = Counter(chain.from_iterable(TrainProcessed['Headline']))\n","TrainFrequencyArticleBody = Counter(chain.from_iterable(TrainProcessed['articleBody']))\n","\n","TestFrequencyHeadline = Counter(chain.from_iterable(TestProcessed['Headline']))\n","TestFrequencyArticleBody = Counter(chain.from_iterable(TestProcessed['articleBody']))\n","\n","ValidationFrequencyHeadline = Counter(chain.from_iterable(ValidationProcessed['Headline']))\n","ValidationFrequencyArticleBody = Counter(chain.from_iterable(ValidationProcessed['articleBody']))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i9x2Zn8-t4CX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for QueryLieklihood\n","def QueryLikelihoodJenlick(dataset,DistributionFrequency,tuningParameter):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryHeadline = {}\n","    UniqueHeadline = []\n","    [UniqueHeadline.append(i) for i in row.Headline if not UniqueHeadline.count(i)]  \n","    \n","    for headlineWord in UniqueHeadline:\n","      DictionaryHeadline[headlineWord]=0 \n","      for bodyWord in row.articleBody:\n","        DictionaryHeadline[bodyWord]=0\n","\n","    for headlineWord in UniqueHeadline:\n","      countWord = row.articleBody.count(headlineWord)\n","      firstTerm = tuningParameter*(countWord/len(row.articleBody))\n","      secondTerm = (1-tuningParameter)*(DistributionFrequency[headlineWord]/sum(DistributionFrequency.values()))\n","      DictionaryHeadline[headlineWord] = firstTerm + secondTerm\n","    DictionaryList.append(DictionaryHeadline)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y-FSnRk92z6S","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for DocumentLikelihood\n","def DocumentLikelihoodJenlick(dataset,DistributionFrequency,tuningParameter):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryBody = {}\n","    UniqueBody = []\n","    [UniqueBody.append(i) for i in row.articleBody if not UniqueBody.count(i)]  \n","\n","    for bodyWord in UniqueBody:\n","      DictionaryBody[bodyWord]=0\n","      for headlineWord in row.Headline:\n","        DictionaryBody[headlineWord]=0 \n","\n","    for bodyWord in UniqueBody:\n","        countWord = row.Headline.count(bodyWord)\n","        firstTerm = tuningParameter*(countWord/len(row.Headline))\n","        secondTerm = (1-tuningParameter)*(DistributionFrequency[bodyWord]/sum(DistributionFrequency.values()))\n","        DictionaryBody[bodyWord] = firstTerm + secondTerm\n","    DictionaryList.append(DictionaryBody)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5UK0OQYBpMkc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","#Get Prbability Distributions for each model\n","TrainQueryLikelihoodJenlick         = QueryLikelihoodJenlick(TrainProcessed,TrainFrequencyArticleBody,i)\n","ValidationQueryLikelihoodJenlick    = QueryLikelihoodJenlick(ValidationProcessed,ValidationFrequencyArticleBody,i)\n","TestQueryLikelihoodJenlick          = QueryLikelihoodJenlick(TestProcessed,TestFrequencyArticleBody,i)\n","\n","TrainDocumentLikelihoodJenlick      = DocumentLikelihoodJenlick(TrainProcessed,TrainFrequencyHeadline,i)\n","ValidationDocumentLikelihoodJenlick = DocumentLikelihoodJenlick(ValidationProcessed,ValidationFrequencyHeadline,i)\n","TestDocumentLikelihoodJenlick       = DocumentLikelihoodJenlick(TestProcessed,TestFrequencyHeadline,i)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6cW38x2NB77d","colab_type":"text"},"cell_type":"markdown","source":["### HYPERPARAMETER TUNING Jelinek-Mercer"]},{"metadata":{"id":"49-zFTwsCCgV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["hyper = np.linspace(0.1,1,10)\n","for i in hyper:\n","  TrainQueryLikelihoodJenlick         = QueryLikelihoodJenlick(TrainProcessed,TrainFrequencyArticleBody,i)\n","  ValidationQueryLikelihoodJenlick    = QueryLikelihoodJenlick(ValidationProcessed,ValidationFrequencyArticleBody,i)\n","  TestQueryLikelihoodJenlick          = QueryLikelihoodJenlick(TestProcessed,TestFrequencyArticleBody,i)\n","\n","  TrainDocumentLikelihoodJenlick      = DocumentLikelihoodJenlick(TrainProcessed,TrainFrequencyHeadline,i)\n","  ValidationDocumentLikelihoodJenlick = DocumentLikelihoodJenlick(ValidationProcessed,ValidationFrequencyHeadline,i)\n","  TestDocumentLikelihoodJenlick       = DocumentLikelihoodJenlick(TestProcessed,TestFrequencyHeadline,i)\n","  \n","     ####ENTER CODE TAKEN\n","  ###FROM PROCESSING AND KL DIVERGENCE BELOW:\n","  ###### REMOVED TO AVOID CLUTTER"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WpzsWaF1DBZY","colab_type":"text"},"cell_type":"markdown","source":["# DIRICHLET SMOOTHING"]},{"metadata":{"id":"7pKlY8cRDCSc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#CREATE FREQUENCY DISTRIBUTION OF ALL WORDS FOR EACH LANGUAGE MODEL! FOR BACKGROUND NOISE\n","\n","TrainFrequencyHeadline = Counter(chain.from_iterable(TrainProcessed['Headline']))\n","TrainFrequencyArticleBody = Counter(chain.from_iterable(TrainProcessed['articleBody']))\n","\n","TestFrequencyHeadline = Counter(chain.from_iterable(TestProcessed['Headline']))\n","TestFrequencyArticleBody = Counter(chain.from_iterable(TestProcessed['articleBody']))\n","\n","ValidationFrequencyHeadline = Counter(chain.from_iterable(ValidationProcessed['Headline']))\n","ValidationFrequencyArticleBody = Counter(chain.from_iterable(ValidationProcessed['articleBody']))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZBhEMu28v04K","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for QueryLieklihood\n","def QueryLikelihoodDirchlect(dataset,DistributionFrequency,newtuningParameter):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryHeadline = {}\n","    UniqueHeadline = []\n","    [UniqueHeadline.append(i) for i in row.Headline if not UniqueHeadline.count(i)]  \n","    \n","    for headlineWord in UniqueHeadline:\n","      DictionaryHeadline[headlineWord]=0 \n","      for bodyWord in row.articleBody:\n","        DictionaryHeadline[bodyWord]=0\n","\n","    for headlineWord in UniqueHeadline:\n","      articleLength = len(row.articleBody)\n","      #print(headlineWord+str(articleLength))\n","      \n","      countWord = row.articleBody.count(headlineWord)\n","      a = articleLength/(articleLength+newtuningParameter)\n","      print(headlineWord +str(a))\n","      firstTerm = a*countWord/articleLength\n","      secondTerm = (1-a)*(DistributionFrequency[headlineWord]/sum(DistributionFrequency.values()))\n","      DictionaryHeadline[headlineWord] = firstTerm + secondTerm\n","    DictionaryList.append(DictionaryHeadline)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rHSs5XnHv04S","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Function for DocumentLikelihood\n","def DocumentLikelihoodDirchlect(dataset,DistributionFrequency,newtuningParameter):\n","  DictionaryList= []\n","  for index, row  in dataset.iterrows():\n","    DictionaryBody = {}\n","    UniqueBody = []\n","    [UniqueBody.append(i) for i in row.articleBody if not UniqueBody.count(i)]  \n","\n","    for bodyWord in UniqueBody:\n","      DictionaryBody[bodyWord]=0\n","      for headlineWord in row.Headline:\n","        DictionaryBody[headlineWord]=0 \n","\n","    for bodyWord in UniqueBody:\n","        countWord = row.Headline.count(bodyWord)\n","        a = len(row.Headline)/(len(row.Headline)+newtuningParameter)\n","        firstTerm = (a)*(countWord/len(row.Headline))\n","        print(str(bodyWord)+str(a))\n","        secondTerm = (1-a)*(DistributionFrequency[bodyWord]/sum(DistributionFrequency.values()))\n","        DictionaryBody[bodyWord] = firstTerm + secondTerm\n","    DictionaryList.append(DictionaryBody)\n","  return DictionaryList"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RuagKvw6cM6f","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Get Prbability Distributions for each model\n","TrainQueryLikelihoodDirchlect        = QueryLikelihoodDirchlect(TrainProcessed,TrainFrequencyArticleBody,215)\n","ValidationQueryLikelihoodDirchlect     = QueryLikelihoodDirchlect(ValidationProcessed,ValidationFrequencyArticleBody,215)\n","TestQueryLikelihoodDirchlect         = QueryLikelihoodDirchlect(TestProcessed,TestFrequencyArticleBody,215)\n","\n","TrainDocumentLikelihoodDirchlect      = DocumentLikelihoodDirchlect(TrainProcessed,TrainFrequencyHeadline,10)\n","ValidationDocumentLikelihoodDirchlect  = DocumentLikelihoodDirchlect(ValidationProcessed,ValidationFrequencyHeadline,10)\n","TestDocumentLikelihoodDirchlect     = DocumentLikelihoodDirchlect(TestProcessed,TestFrequencyHeadline,10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aS6etZ0GEFnt","colab_type":"text"},"cell_type":"markdown","source":["### HYPERPARAMETER TUNING DIRICHLET"]},{"metadata":{"id":"lhj8dQr0AbTg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["hyper = np.linspace(100,200,30)\n","hyper2 = np.linspace(5,10,30)\n","for i in hyper:\n","  TrainQueryLikelihoodDirchlect        = QueryLikelihoodDirchlect(TrainProcessed,TrainFrequencyArticleBody,i)\n","  ValidationQueryLikelihoodDirchlect     = QueryLikelihoodDirchlect(ValidationProcessed,ValidationFrequencyArticleBody,i)\n","  TestQueryLikelihoodDirchlect         = QueryLikelihoodDirchlect(TestProcessed,TestFrequencyArticleBody,i)\n","\n","  TrainDocumentLikelihoodDirchlect      = DocumentLikelihoodDirchlect(TrainProcessed,TrainFrequencyHeadline,j)\n","  ValidationDocumentLikelihoodDirchlect  = DocumentLikelihoodDirchlect(ValidationProcessed,ValidationFrequencyHeadline,j)\n","  TestDocumentLikelihoodDirchlect     = DocumentLikelihoodDirchlect(TestProcessed,TestFrequencyHeadline,j)\n","  \n","    ####ENTER CODE TAKEN\n","  ###FROM PROCESSING AND KL DIVERGENCE BELOW:\n","  ###### REMOVED TO AVOID CLUTTER"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O535xNu9Etmb","colab_type":"text"},"cell_type":"markdown","source":["# PROCESSING AND FINAL KL DIVERGENCE"]},{"metadata":{"id":"OgcZ7oP4wvyt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["trainLikelihoods = pd.DataFrame(list(zip(TrainQueryLikelihoodDiscounting, TrainDocumentLikelihoodDiscounting )), columns = ['qldict', 'dldict'])\n","validationLikelihoods = pd.DataFrame(list(zip(ValidationQueryLikelihoodDiscounting, ValidationDocumentLikelihoodDiscounting )), columns = ['qldict', 'dldict'])\n","testLikelihoods = pd.DataFrame(list(zip(TestQueryLikelihoodDiscounting, TestDocumentLikelihoodDiscounting )), columns = ['qldict', 'dldict'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xcp-dAsQLT8q","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def DictionaryToValues(DictionaryOfProbabilities):\n","  querylikelihoodsValues = []\n","  documentlikelihoodsValues = []\n","  for index,row in DictionaryOfProbabilities.iterrows():\n","    querylikelihoodtemplist = []\n","    documentlikelihoodtemplist = []\n","    for k in row.qldict.keys() & row.dldict.keys():\n","      querylikelihoodtemplist.append(row.qldict[k])\n","      documentlikelihoodtemplist.append(row.dldict[k])\n","    querylikelihoodsValues.append(querylikelihoodtemplist)\n","    documentlikelihoodsValues.append(documentlikelihoodtemplist)\n","  return querylikelihoodsValues, documentlikelihoodsValues\n","    \n","    \n","    #print(k, row.qldict[k], row.dldict[k] )\n","    \n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"IFIabDzGberj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["TrainQueryLikelihoodsValues, TrainDocumentLikelihoodsValues = DictionaryToValues(trainLikelihoods)\n","ValidationQueryLikelihoodsValues, ValidationDocumentLikelihoodsValues = DictionaryToValues(validationLikelihoods)\n","TestQueryLikelihoodsValues, TestDocumentLikelihoodsValues = DictionaryToValues(testLikelihoods)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BVfqfX0pcs7C","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["trainLikelihoodValues = pd.DataFrame(list(zip(TrainQueryLikelihoodsValues, TrainDocumentLikelihoodsValues )), columns = ['qlValues', 'dlValues'])\n","validationLikelihoodValues = pd.DataFrame(list(zip(ValidationQueryLikelihoodsValues, ValidationDocumentLikelihoodsValues )), columns = ['qlValues', 'dlValues'])\n","testLikelihoods=Values = pd.DataFrame(list(zip(TestQueryLikelihoodsValues, TestDocumentLikelihoodsValues )), columns = ['qlValues', 'dlValues'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HEZGeJCy2j-c","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["TrainWithLikelihoods = pd.concat([TrainProcessed, trainLikelihoodValues], axis=1, join_axes=[TrainProcessed.index])\n","ValidationWithLikelihoods = pd.concat([ValidationProcessed, validationLikelihoodValues], axis=1, join_axes=[ValidationProcessed.index])\n","TestWithLikelihoods = pd.concat([TestProcessed, testLikelihoods], axis=1, join_axes=[TestProcessed.index])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MXB_kGEsNiEV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def KL(a, b):\n","    a = np.asarray(a, dtype=np.float)\n","    b = np.asarray(b, dtype=np.float)\n","\n","    return np.sum(np.where(a != 0, a * np.log(a / b), 0))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yXCaB0RZdqvz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["TrainWithLikelihoods['KLDivergence'] = TrainWithLikelihoods.apply(lambda x: KL(x['qlValues'], x['dlValues']), axis=1)\n","ValidationWithLikelihoods['KLDivergence'] = ValidationWithLikelihoods.apply(lambda x: KL(x['qlValues'], x['dlValues']), axis=1)\n","TestWithLikelihoods['KLDivergence'] = TestWithLikelihoods.apply(lambda x: KL(x['qlValues'], x['dlValues']), axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Slq0tfGIsQY1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["ValidationWithLikelihoods.groupby('Stance').mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W4TahU8KjybA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["TrainWithLikelihoods.groupby('Stance').mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fkd4yf5AhVVm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["ValidationWithLikelihoods.groupby('Stance').mean()\n","TESTINGPRECISION = TrainWithLikelihoods[TrainWithLikelihoods.Stance != 'unrelated']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KDwHRtqsCMq2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["groups = TESTINGPRECISION.groupby('Stance')\n","\n","# Plot\n","fig, ax = plt.subplots()\n"," # Optional, just adds 5% padding to the autoscaling\n","for name, group in groups:\n","    ax.plot(group.CosineSim, group.KLDivergence, marker='o', linestyle='', ms=4, label=name)\n","ax.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vyORVIAqW3W9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["  #KL DIVERGENCE TESTINGS:\n","#array1 = ['bog' ,'bag' 'rat']\n","#array2 = ['hello' , ''word', 'test' ]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LMgyQxtb_erV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}